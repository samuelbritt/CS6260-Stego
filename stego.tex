\documentclass[11pt]{article}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage[alsoload=binary]{siunitx}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{fancyvrb}
\usepackage[section]{placeins}
\usepackage{flafter}
\usepackage{microtype}
\usepackage{url}
\usepackage{hyperref}

\usepackage{mathpazo}

% a bit more compact
\renewcommand\l{\mathopen{}\left}
\renewcommand\r{\right}

\newcommand\abs[1]{\l\vert #1 \r\vert}

% no section numbers
\setcounter{secnumdepth}{-2}

% leave notes to yourself
\newcommand\todo[1]{\textcolor{red}{\textsc{todo}: #1}}

\input{header}

\newcommand\channel{\ensuremath{\mathcal C}}
\newcommand\sschac{\attack{ss-cha-$\channel$}}

% Steganography
\newcommand\stg{\scheme{S}}
\newcommand\stgenc{\algo{SE}}
\newcommand\stgdec{\algo{SD}}

\title{Analysis of Stegosystems}
\author{Sam Britt, Tushar Humbe, Ben Perry, Sanchita Vijayvargiya}
\date{December 7, 2012}

\begin{document}
\maketitle
\section{Introduction}

\section{Security Definitions}
\subsection{Primitives}
Let $P_X$ be a probability mass function with support $\chi$, where $X$
is a discrete random variable taking the values in $\chi$. The
\emph{entropy} of $X$ is
\begin{equation*}
  H(X) = E\l( -\lg P_X \r),
\end{equation*}
where $E(\cdot)$ is the expected value (weighted average) function;
that is,
\begin{equation}
  H(X) = - \sum_{x\in \chi} P_X(x) \lg P_X(x).
  \label{eq:entropy}
\end{equation}
Intuitively, the entropy of $X$ is a measure of the number of bits of
uncertainty in $X$. For example, suppose $\chi$ is the set of all
$n$-bit strings, and $P_X(x) = 1 / 2^n$ for any $x \in \chi$; that
is, every $n$-bit string is equally likely to be pulled from $P_X$.
This would represent a distribution of maximum uncertainty, and it is
straightforward to show that Eqn.~\eqref{eq:entropy} evaluates to $n$
in this case. In fact, $H(X) = \lg \abs{\chi}$ is an upper bound for
$H$, where $\abs \chi$ denotes the cardinality of $\chi$.

The \emph{minimum entropy} of a distribution $P_X$ is defined as
\begin{equation}
  H_\infty\l( X \r) = \min_{x \in \chi} \l\{ -\lg P_X(x) \r\}
  \label{eq:min-entropy}
\end{equation}
This can be understood as a measure of uncertainty for the ``most
probable'' element in $\chi$ according to $P_X$. For example, if there
is some element $x_0$ with $P_X(x_0) = 1$, then $H_\infty(X) = 0$
(there is no uncertainty in $X$). Suppose the most probable element
$x_0$ has probability $P_X(x_0) = 1/2$. Intuitively, the uncertainty
is unity; that is, we can guess that the next value of $X$ will be
$x_0$ to within a single coin flip. Indeed, evaluating
Eqn.~\eqref{eq:min-entropy} for such a distribution shows that
$H_\infty\l( X \r) = 1$.

Given two probability mass functions $P_X$ and $P_Y$, both with
support $\chi$, the \emph{relative entropy}, also called the
Kullback-Leibler divergence, from $P_X$ to $P_Y$ is defined to be
\begin{equation}
  D \l( P_X \concat P_Y \r) =
  \sum_{x \in \chi} P_X(x) \lg \frac{P_X(x)}{P_Y(x)}.
\end{equation}
Intuitively, the relative entropy is a measure of the difference
between $P_X$ and $P_Y$, although it is important to note that it is
not symmetric; that is, $D \l( P_X \concat P_Y \r) \ne D \l( P_Y
\concat P_X \r)$. However, $D \l( P_X \concat P_Y \r) = 0 $ if and
only if $P_X = P_Y$, and increases indefinitely as $P_Y$ diverges from
$P_X$.

A \emph{channel} as defined by \todo{cite} is a distribution on
timestamped bit sequences; i.e., a channel \channel\ is a distribution
with support $\set{\l( \bit, t_1 \r), \l( \bit, t_2 \r), \ldots }$,
where each $t_i \le t_{i+1}$. The intent is to model communication,
where not just the content but also the timing of the communication
may be relevant. Since a particular draw from the distribution
\channel\ depends the history of previously drawn bits, define
$\channel_h$ to be the distribution conditioned on history $h$.
Furthermore, it is useful to think of drawing from the channel in
chunks of $b$ bits at a time, so define $\channel_h^b$ to be the
distribution on the next $b$ bits after the history $h$.

\subsection{Security Definitions}
Hopper, et. al \todo{cite} define a stegosystem $\stg$ as a pair of
randomized algorithms $\l( \stgenc, \stgdec \r)$. $\stgenc$ takes as
input a shared key $k$, a hiddentext message $m$, and a message
history $h$, and an oracle $M(h)$ that samples according to channel
distribution $\channel_h^b$, where channels are required to satisfy,
for all $h$ drawn from \channel, $H_\infty\l( \channel_h^b \r) > 1$.
As output, $\stgenc_k^M\l( m, h \r)$ returns a sequence $c_1 \concat
c_2 \concat \ldots \concat c_\ell$ in the support of $\channel_h^{\ell
b}$. The decryption algorithm $\stgdec_k^M\l( c_1 \concat c_2 \concat
\ldots \concat c_\ell, h \r)$ returns a message $m$, which should be
``correct''; that is, the same message encoded by $\stgenc$, at least
$2/3$ of the time.

The authors define the security of a stegosystem in terms of a game.
The adversarial warden $W$ is given access to $M\l( h \r)$, which
returns draws from $\channel_h^b$, and an oracle $\Ora$. The oracle
$\Ora$ is either $\stgenc_k$ or a function $O(\cdot, \cdot)$, where
$O(m, h)$ simply returns a draw from $\channel_h^{\abs{\stgenc_k\l( m,
h \r)}}$. The warden also has access to randomness $r$. The warden's
advantage against the steganographic secrecy under chosen hiddentext
attack for channel $\channel$ of stegosystem $\stg$ is defined by
Hopper et.\ al to be
\begin{align*}
  \advan_{\stg, \channel}^\sschac \l( W \r) = \abs {
  \Pr_{k, r, M, \stgenc} \l[ W_r ^{M, \stgenc_k(\cdot, \cdot)}
  \text{accepts} \r]
  -
  \Pr_{r, M, O} \l[ W_r ^{M, O(\cdot, \cdot)}
  \text{accepts} \r]
}.
\end{align*}
A stegosystem $\stg$ is \emph{$(t, q, \ell,
\epsilon)$-steganographically secret under chosen hiddenttext attack}
for channel $\channel$ (SS-CHA-$\channel$) if, for any warden $W$
making at most $q$ queries totaling at most $\ell$ bits of hiddentext,
and running in time at most $t$,
\begin{equation*}
  \advan_{\stg, \channel}^\sschac\l( W \r) \le \epsilon;
\end{equation*}
that is, the stegosystem $\stg$ is insecure if an efficient warden can
(with high probability) distinguish between the output of
$\stgenc_k(m, h)$ and draws from $\channel_h^{\abs{\stgenc_k(m,h)}}$,
even when given access to $\channel_h^b$ through $M$. A stegosystem
$\stg$ is \emph{$(t, q, \ell, \epsilon)$-universally
steganographically secret under chosen hiddenttext attack} for channel
$\channel$ (USS-CHA-$\channel$) if it is $\l( t, q, \ell,
\epsilon\r)$-SS-CHA-\channel\ for any channel \channel\ that
satisfies, $\forall h$ drawn from \channel, $H_\infty\l( \channel_h^b
\r) > 1$.

Cachin \todo{cite} takes an information theoretic approach to
steganographic security. He considers the basic prisoner's problem,
where Alice sends either an innocent covertext or a stegotext
concealing a message to Bob over an open communication line. The
warden eavesdrops on the line, and must decide whether the
communication is a covertext or stegotext. Let $P_C$ be the
distribution of covertexts, and let $P_S$ be the distribution of
stegotexts; these distributions are known to the warden. Cachin
defines the overall security of the system in terms of the relative
entropy between $P_C$ and $P_S$; namely, the stegosystem is
\emph{information-theoretic perfectly secure} if
\begin{equation*}
  D\l( P_C \concat P_S \r) = 0,
\end{equation*}
and is \emph{information-theoretic $\epsilon$-secure} if
\begin{equation*}
  D\l( P_C \concat P_S \r) \le \epsilon.
\end{equation*}
Intuitively, Cachin is claiming that a stegosystem is secure if the
probability distributions of covertexts and stegotexts are ``close,"
so that, given a message in the support of $P_C$ and $P_S$, the
warden has very little reason to believe it was drawn from one
distribution over the other. Cachin goes on to analyze the decision
from the framework of hypothesis testing.

Cachin's warden is a much weaker adversary than Hopper's, and
correspondingly, Hopper's SS-CHA-\channel\ game provides a much
stronger definition of security. Some key weaknesses in Cachin's
definition:
\begin{itemize}
  \item Cachin's warden receives a single message over the
    communication line, and must determine her decision. In contrast,
    Hopper's warden has access to an oracle that can be queried as
    much as is computationally feasible. Indeed, Cachin's model is
    roughly equivalent to the SS-CHA-\channel\ game when only a single
    query is allowed.
  \item Cachin's warden knows the distribution of possible messages
    chosen by Alice, but does not know the message. Hopper, however,
    allows for an interactive, chosen hiddentext attack.
  \item Hopper's channels are all conditioned on the \emph{history} of
    previously drawn samples. And since his warden is allowed to
    specify arbitrary history to the oracles, a stegosystem that meets
    SS-CHA-\channel\ security should be secure for any valid history
    of communication on \channel. On the other hand, Cachin's model,
    because it depends on static probability distributions, does not
    capture this sequential element of real communication. Even if
    $P_C$ and $P_S$ were themselves dependent on history, Cachin's
    model would only be able to claim security of the stegosystem for
    the particular history leading up to his experiment.
\end{itemize}

\subsection{Good Stego}

\section{Bad Stego}
\section{Conclusion}
\section{References}

\end{document}
